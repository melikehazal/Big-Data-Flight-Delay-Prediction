{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6a67e0-7211-41f3-835d-27a1acc41a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark hazır, S3A ve Kafka konektörleri yüklendi: <pyspark.sql.session.SparkSession object at 0x000002D3DFAFBF90>\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_PYTHON\"]        = sys.executable\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"  \n",
    "os.environ[\"PATH\"]        = os.environ[\"HADOOP_HOME\"] + r\"\\bin;\" + os.environ[\"PATH\"]\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]     = \"AKIA2O2UYWBHSGHSH6WD\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"]    = \"eu-north-1\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDelayKafkaTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "        \",\".join([\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.499\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\"\n",
    "        ])\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",               \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\",         os.environ[\"AWS_ACCESS_KEY_ID\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\",         os.environ[\"AWS_SECRET_ACCESS_KEY\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\",           \"s3.eu-north-1.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\",  \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\",        \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"array\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.disable\",       \"true\") \\\n",
    "    .config(\"spark.pyspark.python\",        sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark hazır, S3A ve Kafka konektörleri yüklendi:\", spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266be3fa-8b3f-4934-ba99-0510114f4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modeller yüklendi\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "clf_model  = PipelineModel.load(\"s3a://ceng476/models/model_classification_v1\")\n",
    "norm_model = PipelineModel.load(\"s3a://ceng476/models/model_normal_v1\")\n",
    "ext_model  = PipelineModel.load(\"s3a://ceng476/models/model_extreme_v1\")\n",
    "\n",
    "print(\"✅ Modeller yüklendi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d624bb-61de-4732-92db-64a296e1da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KafkaConsumer hazır, topic: {'test_topic'}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'test_topic',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    group_id='batch-predict-group',  \n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=False,\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    ")\n",
    "print(\"✅ KafkaConsumer hazır, topic:\", consumer.subscription())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc589ba4-8081-48cf-83de-27d34839163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema hazır\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DoubleType\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"year\",                IntegerType(), True),\n",
    "    StructField(\"month\",               IntegerType(), True),\n",
    "    StructField(\"carrier\",             StringType(),  True),\n",
    "    StructField(\"carrier_name\",        StringType(),  True),\n",
    "    StructField(\"airport\",             StringType(),  True),\n",
    "    StructField(\"airport_name\",        StringType(),  True),\n",
    "    StructField(\"arr_flights\",         DoubleType(),  True),\n",
    "    StructField(\"arr_del15\",           DoubleType(),  True),\n",
    "    StructField(\"carrier_ct\",          DoubleType(),  True),\n",
    "    StructField(\"weather_ct\",          DoubleType(),  True),\n",
    "    StructField(\"nas_ct\",              DoubleType(),  True),\n",
    "    StructField(\"security_ct\",         DoubleType(),  True),\n",
    "    StructField(\"late_aircraft_ct\",    DoubleType(),  True),\n",
    "    StructField(\"arr_cancelled\",       DoubleType(),  True),\n",
    "    StructField(\"arr_diverted\",        DoubleType(),  True),\n",
    "    StructField(\"arr_delay\",           DoubleType(),  True),\n",
    "    StructField(\"carrier_delay\",       DoubleType(),  True),\n",
    "    StructField(\"weather_delay\",       DoubleType(),  True),\n",
    "    StructField(\"nas_delay\",           DoubleType(),  True),\n",
    "    StructField(\"security_delay\",      DoubleType(),  True),\n",
    "    StructField(\"late_aircraft_delay\", DoubleType(),  True),\n",
    "])\n",
    "print(\"✅ Schema hazır\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4609e768-1821-4b00-a9d4-908b174f5fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ process_next_batch() fonksiyonu tekrar güncellendi\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "def process_next_batch():\n",
    "    # 1) Kafka’dan BATCH_SIZE kayıt oku\n",
    "    records = []\n",
    "    for i, msg in enumerate(consumer):\n",
    "        records.append(msg.value)\n",
    "        if i+1 >= BATCH_SIZE:\n",
    "            break\n",
    "    if not records:\n",
    "        print(\"⚠️ Yeni mesaj yok, birkaç sn bekleniyor...\")\n",
    "        return\n",
    "    \n",
    "    # 2) DataFrame’e dönüştür\n",
    "    df = spark.createDataFrame(records, schema)\n",
    "    \n",
    "    # 3) ETL: feature engineering\n",
    "    df = (df\n",
    "        .withColumn(\"is_summer\",        when((col(\"month\") >= 6)&(col(\"month\") <= 8), 1).otherwise(0))\n",
    "        .withColumn(\"is_winter\",        when((col(\"month\") == 12)|(col(\"month\") <= 2), 1).otherwise(0))\n",
    "        .withColumn(\"is_holiday_month\", when((col(\"month\") == 12)|(col(\"month\") == 7), 1).otherwise(0))\n",
    "        .withColumn(\"delay_ratio\",      when(col(\"arr_flights\")>0, col(\"arr_del15\")/col(\"arr_flights\")).otherwise(0))\n",
    "        .withColumn(\"avg_delay_per_flight\",\n",
    "            when(col(\"arr_flights\")>0, col(\"arr_delay\")/col(\"arr_flights\")).otherwise(0))\n",
    "    )\n",
    "    \n",
    "    # 4) Classification\n",
    "    step1 = clf_model.transform(df)\n",
    "    \n",
    "    # 5) Çakışma çıkaran indexed/encoded ve ara ML sütunlarını listele\n",
    "    indexed_cols = [c for c in step1.columns if c.endswith(\"_indexed\") or c.endswith(\"_encoded\")]\n",
    "    ml_cols = [\n",
    "        \"features\",\n",
    "        \"scaledFeatures\",    # camelCase\n",
    "        \"scaled_features\",   # snake_case — pipeline bunu da kullanıyorsa\n",
    "        \"rawPrediction\",\n",
    "        \"probability\"\n",
    "    ]\n",
    "    drop_cols = indexed_cols + ml_cols + [\"prediction\"]\n",
    "    \n",
    "    # 6) Normal/Extreme’a ayırırken bu sütunları drop et\n",
    "    norm_raw = (\n",
    "        step1\n",
    "          .filter(col(\"prediction\")==0)\n",
    "          .drop(*drop_cols)\n",
    "    )\n",
    "    extreme_raw = (\n",
    "        step1\n",
    "          .filter(col(\"prediction\")==1)\n",
    "          .drop(*drop_cols)\n",
    "    )\n",
    "    \n",
    "    # 7) Regressor pipeline’larını çalıştır\n",
    "    norm_preds = norm_model.transform(norm_raw)\n",
    "    ext_preds  = ext_model.transform(extreme_raw)\n",
    "    \n",
    "    result = (\n",
    "        norm_preds.unionByName(ext_preds)\n",
    "                  .withColumnRenamed(\"prediction\",\"arr_delay_pred\")\n",
    "    )\n",
    "    \n",
    "    # 8) Tahminleri göster ve offset commit et\n",
    "    print(\"=== Yeni Batch Tahminleri ===\")\n",
    "    result.select(\"year\",\"month\",\"carrier\",\"airport\",\"arr_delay\",\"arr_delay_pred\").show(truncate=False)\n",
    "    consumer.commit()\n",
    "\n",
    "print(\"✅ process_next_batch() fonksiyonu tekrar güncellendi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "312c830d-29e1-4822-b376-b92b95f1d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Yeni Batch Tahminleri ===\n",
      "+----+-----+-------+-------+---------+------------------+\n",
      "|year|month|carrier|airport|arr_delay|arr_delay_pred    |\n",
      "+----+-----+-------+-------+---------+------------------+\n",
      "|2024|12   |MQ     |EVV    |732.0    |656.3149914726639 |\n",
      "|2024|12   |MQ     |EWR    |2531.0   |2367.9269387524064|\n",
      "|2024|12   |MQ     |EYW    |1596.0   |1423.1794838665755|\n",
      "|2024|12   |MQ     |GNV    |419.0    |497.8440154440423 |\n",
      "|2024|12   |MQ     |GRK    |1635.0   |1868.2891253356208|\n",
      "|2024|12   |MQ     |GRR    |1584.0   |1615.6472630142368|\n",
      "|2024|12   |MQ     |GSP    |1380.0   |1397.5283060512372|\n",
      "|2024|12   |MQ     |ILM    |1479.0   |1387.561442766342 |\n",
      "|2024|12   |MQ     |LAX    |394.0    |438.63741033966437|\n",
      "|2024|12   |MQ     |LEX    |1399.0   |1415.073698566547 |\n",
      "|2024|12   |MQ     |LSE    |24.0     |20.17409910553865 |\n",
      "|2024|12   |MQ     |MAF    |2584.0   |2643.8130839613336|\n",
      "|2024|12   |MQ     |MEM    |1917.0   |1840.9930185324235|\n",
      "|2024|12   |MQ     |MFE    |530.0    |615.5324483760468 |\n",
      "|2024|12   |MQ     |OMA    |633.0    |531.5408643967813 |\n",
      "|2024|12   |MQ     |ONT    |152.0    |200.19987041878377|\n",
      "|2024|12   |MQ     |ORF    |242.0    |359.9958684446718 |\n",
      "|2024|12   |MQ     |RDM    |2093.0   |2651.733828276845 |\n",
      "|2024|12   |MQ     |RNO    |0.0      |8.345973195488403 |\n",
      "|2024|12   |MQ     |SAV    |1037.0   |986.7099715910307 |\n",
      "+----+-----+-------+-------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87917635-9bd0-4c92-bd39-e14e72d65cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981aeae-b372-4bff-b86c-980cee5bdb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
